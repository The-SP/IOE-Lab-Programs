{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Embedding**\n",
    "- Create count based word embedding\n",
    "- One hot embedding\n",
    "- Word embedding using Gensim\n",
    "    - Create a word2vec model using text from wikipedia https://en.wikipedia.org/wiki/Machine_learning\n",
    "    - what are possible preprocessing\n",
    "    - tokenization?\n",
    "    - what is the trained vocab?\n",
    "    - how to treat words not in vocab?\n",
    "    - length of vector?\n",
    "- interpret the output of the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_docs = [\"The economic slowdown is becoming more severe\",\n",
    "           \"The movie was simply awesome\",\n",
    "           \"I like cooking my own food\",\n",
    "           \"Samsung is announcing a new technology\",\n",
    "           \"Machine Learning is an example of awesome technology\",\n",
    "           \"All of us were excited at the movie\",\n",
    "           \"We have to do more to reverse the economic slowdown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count-based Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all' 'an' 'announcing' 'at' 'awesome' 'becoming' 'cooking' 'do'\n",
      " 'economic' 'example' 'excited' 'food' 'have' 'is' 'learning' 'like'\n",
      " 'machine' 'more' 'movie' 'my' 'new' 'of' 'own' 'reverse' 'samsung'\n",
      " 'severe' 'simply' 'slowdown' 'technology' 'the' 'to' 'us' 'was' 'we'\n",
      " 'were']\n",
      "Count-based Word Embedding:\n",
      "[[0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 2 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(my_docs)\n",
    "\n",
    "# Vocabulary\n",
    "vocab = count_vectorizer.get_feature_names_out()\n",
    "print(vocab)\n",
    "\n",
    "# Count-based word embedding\n",
    "count_embedding = count_matrix.toarray()\n",
    "\n",
    "print(\"Count-based Word Embedding:\")\n",
    "print(count_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Embedding:\n",
      "[[0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Create a CountVectorizer instance for one-hot encoding\n",
    "count_vectorizer = CountVectorizer(binary=True)\n",
    "one_hot_matrix = count_vectorizer.fit_transform(my_docs)\n",
    "\n",
    "# Vocabulary\n",
    "vocab = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# One-hot embedding\n",
    "one_hot_embedding = one_hot_matrix.toarray()\n",
    "\n",
    "print(\"One-Hot Embedding:\")\n",
    "print(one_hot_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding using Gensim (Word2Vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Embedding:\n",
      "Embedding for the word: computer\n",
      "[ 3.35471542e-03 -9.13533848e-03  4.59504547e-03  4.63672820e-03\n",
      " -7.76442373e-03 -8.24694987e-03 -7.79009005e-03 -7.75163900e-03\n",
      "  1.76021364e-03  2.73724040e-03 -9.59737413e-03  6.85713626e-03\n",
      " -1.05297026e-04 -4.79714992e-03 -5.18830912e-03  9.33995179e-05\n",
      "  8.09609052e-03  5.34964586e-03  8.75388179e-03 -6.34399801e-03\n",
      " -1.98542210e-03 -1.77396648e-03 -7.58295925e-03  1.84197666e-03\n",
      " -7.67881237e-03  5.72665315e-03 -8.51520337e-03  1.45647756e-03\n",
      " -3.90171044e-06 -7.10296445e-03  8.20576865e-03  6.60096295e-03\n",
      "  3.34269600e-03  6.19508931e-03 -7.13171787e-04  4.48410353e-03\n",
      " -2.08885851e-03  4.66461061e-03  2.40713079e-03 -9.11064795e-04\n",
      "  4.54581914e-06  2.97173741e-03 -5.67242922e-03  9.98330116e-03\n",
      " -7.04643503e-03  9.54082515e-03  5.69678890e-03 -6.27642963e-03\n",
      " -5.32261282e-03  2.75894068e-03 -5.74489310e-03  7.32901599e-03\n",
      "  7.07960362e-03 -8.79097078e-03  3.09709483e-03  1.52279122e-03\n",
      " -1.19015633e-03 -7.68693443e-03 -7.32205436e-03 -6.30334858e-03\n",
      " -8.04331526e-03 -2.83253402e-03 -4.81232861e-03  1.13982940e-03\n",
      " -4.09263559e-03  8.62059824e-04 -5.26139559e-03  1.21186320e-04\n",
      " -6.54001115e-03  9.19476803e-03  4.04454954e-03  1.84312565e-04\n",
      " -1.06603245e-03  4.67686681e-03  4.84286575e-03 -3.31690302e-03\n",
      " -9.72211268e-03  4.90096863e-03 -7.59848102e-04  4.95289825e-03\n",
      " -5.51722990e-03  3.40071623e-03 -8.09903780e-04  5.11527574e-03\n",
      " -6.60608115e-04  1.45526801e-03  7.80325895e-03  7.11579854e-03\n",
      "  9.62442602e-04  6.64496655e-03  1.32453628e-03 -8.07535835e-03\n",
      "  3.35790357e-03 -1.03796599e-03 -7.28845131e-03 -7.57465279e-03\n",
      " -3.38313798e-03  9.00173280e-03 -3.27772740e-03 -1.31037238e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Suraj\n",
      "[nltk_data]     Pathak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data for tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"\"\"\n",
    "Machine learning (ML) is an umbrella term for solving problems for which development of algorithms by human programmers would be cost-prohibitive, and instead the problems are solved by helping machines 'discover' their 'own' algorithms, without needing to be explicitly told what to do by any human-developed algorithms. Recently, generative artificial neural networks have been able to surpass results of many previous approaches. Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.\n",
    "The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.\n",
    "ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically-based, computational statistics is an important source of the field's methods.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocessing: Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Create and train Word2Vec model\n",
    "model = Word2Vec(sentences=[tokens], vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Vocabulary\n",
    "vocab = list(model.wv.index_to_key)\n",
    "\n",
    "# Treat words not in vocabulary as \"unknown\"\n",
    "out_of_vocab_word = \"computer\"\n",
    "if out_of_vocab_word in model.wv:\n",
    "    embedding = model.wv[out_of_vocab_word]\n",
    "else:\n",
    "    embedding = None\n",
    "\n",
    "print(\"Word2Vec Embedding:\")\n",
    "print(\"Embedding for the word:\", out_of_vocab_word)\n",
    "print(embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
